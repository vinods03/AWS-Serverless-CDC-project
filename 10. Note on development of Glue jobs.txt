A note on how the Glue jobs were developed.

The spark script was developed on the local laptop.
Refer C:\Vinod\SparkLocalDevelopment\Code\app_5.py.
The files delivered by DMS into S3 were downloaded to the local laptop and development was done using these 2 local files.

Refer C:\Vinod\AWS DataEngineering\5. AWS pyspark course\1. GOOD Local Environment Setup (from another course Spark Streaming).txt for the steps on how to setup hadoop/spark on local laptop.

Once local script is working, went to the Glue console -> Spark script editor -> write or upload your own code
Put your locally developed code between job.init and job.commit.
Make sure you refer to S3 locations for the data and not your local machine locations. 

I chose to split the locally developed job into 2 Glue jobs - but this is not actually needed.
Once the Glue job is working you can choose to make logical modifications for cleaner design / cleaner code etc.